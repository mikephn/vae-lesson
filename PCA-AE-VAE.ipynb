{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's load a sample dataset with 82k cells from a flow cytometry experiment.\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "tds = pickle.load(open('flow-sample.pkl', \"rb\"))\n",
    "X = tds['X'] # the raw values (approximately normally distributed)\n",
    "Y = tds['Y'] # cell-type labels for reference\n",
    "markers = tds['markers'] # meaning of columns (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare a function for making scatter plots of latent space embeddings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "classes = sorted(list(set(Y)))\n",
    "colormap = plt.cm.hsv\n",
    "class_colors = [colormap(float(ci)/len(classes)) for ci in range(len(classes))]\n",
    "legend_handles = [mpatches.Patch(color=class_colors[ci], label=classes[ci]) for ci in range(len(classes))]\n",
    "colors = [class_colors[classes.index(y)] for y in Y] # colour vector corresponding to Y\n",
    "\n",
    "def plot(Z, title):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(Z[:, 0], Z[:, 1], c=colors, s=0.1)\n",
    "    plt.legend(handles=legend_handles)\n",
    "    plt.xlabel('dim 1')\n",
    "    plt.ylabel('dim 2')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TruncatedSVD implements PCA without the need to center the data\n",
    "# and only calculates the number of dimensions you specify (e.g. n_components=2).\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pca = TruncatedSVD(n_components=2)\n",
    "Z_pca = pca.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(Z_pca, 'Cell types - PCA (first 2 components)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using Keras to make an autoencoder\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The simplest autoencoder has one linear layer for encoding, and one linear layer for decoding.\n",
    "# It's worth using the explicit Functional API to keep references to different sub-parts of the network.\n",
    "\n",
    "# Instantiate the layer tensors:\n",
    "\n",
    "inp = Input(shape=(X.shape[1],)) # input for X\n",
    "enc = Dense(units=2, activation='linear') # for encoding to Z\n",
    "dec = Dense(units=X.shape[1], activation='linear') # for decoding back to X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connecting layers is done through nested function call syntax on tensors:\n",
    "\n",
    "reconstruction = dec(enc(inp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Keras Model object from connected layers to be able to predict from the stack.\n",
    "\n",
    "linear_AE = keras.Model(inputs=inp, outputs=reconstruction)\n",
    "linear_AE.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple training in Keras using SGD optimiser and MSE reconstruction loss:\n",
    "\n",
    "linear_AE.compile(optimizer='sgd', loss='mse')\n",
    "linear_AE.fit(X, X, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# (change number of epochs and batch size if you're not satisfied with the loss convergence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To predict Z from X we want a model from the first input to the output of encoder, using existing layers:\n",
    "\n",
    "linear_encoder = keras.Model(inp, enc(inp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Call predict to get latent embedding in Z:\n",
    "\n",
    "Z_ae_lin = linear_encoder.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(Z_ae_lin, 'Cell types - linear autoencoder (2D)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A deep autoencoder with two hidden layers in both encoder and decoder.\n",
    "# You can experiment with the number of hidden layers, number of units, types of hidden activation.\n",
    "# Because there are layers which we don't need to directly access afterwards,\n",
    "# we can stack them right away and only keep references to important endpoints:\n",
    "\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "hidden = Dense(units=128, activation='elu')(inp)\n",
    "hidden = Dense(units=128, activation='elu')(hidden)\n",
    "enc = Dense(units=2, activation='linear')(hidden)\n",
    "hidden = Dense(units=128, activation='elu')(enc)\n",
    "hidden = Dense(units=128, activation='elu')(hidden)\n",
    "reconstruction = Dense(units=X.shape[1], activation='linear')(hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AE = keras.Model(inp, reconstruction)\n",
    "AE.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AE.compile(optimizer='sgd', loss='mse')\n",
    "AE.fit(X, X, epochs=10, batch_size=32, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = keras.Model(inp, enc)\n",
    "Z_ae = encoder.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(Z_ae, 'Cell types - deep autoencoder (2D)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VAE\n",
    "import tensorflow as tf\n",
    "\n",
    "# This custom layer returns a random sample from a normal distribution of provided mean and sd.\n",
    "# Log variance is input for numerical stability and turned to standard deviation.\n",
    "\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # assuming two inputs, one for mean, one for log variance\n",
    "        z_mean, z_log_var = inputs\n",
    "        # sample from unit Gaussian:\n",
    "        epsilon = keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        # reparameterise to mean + sd * N(0, 1)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inp = Input(shape=(X.shape[1],))\n",
    "hidden = Dense(units=128, activation='elu')(inp)\n",
    "hidden = Dense(units=128, activation='elu')(hidden)\n",
    "\n",
    "# Here we have two parallel layers, first predicting mean, second predicting log variance for each latent dimension\n",
    "enc_mean = Dense(units=2, activation='linear')(hidden)\n",
    "enc_log_var = Dense(units=2, activation='linear')(hidden)\n",
    "# and the sampling layer which turns the two parameters into a single probabilistic value in Z.\n",
    "enc = Sampling()([enc_mean, enc_log_var])\n",
    "\n",
    "# decoder takes the probabilistic output and projects it back to data space:\n",
    "hidden = Dense(units=128, activation='elu')(enc)\n",
    "hidden = Dense(units=128, activation='elu')(hidden)\n",
    "reconstruction = Dense(units=X.shape[1], activation='linear')(hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VAE = keras.Model(inp, reconstruction)\n",
    "VAE.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Losses are also tensors and can be created explicitly.\n",
    "# Create the two loss components that sum in the ELBO.\n",
    "\n",
    "# Reconstruction term:\n",
    "rec_loss = tf.reduce_mean(keras.losses.mse(inp, reconstruction))\n",
    "\n",
    "# and the KL divergence regularisation term:\n",
    "kl_loss = -0.5 * tf.reduce_mean(enc_log_var - tf.square(enc_mean) - tf.exp(enc_log_var) + 1)\n",
    "\n",
    "# Adjust the amount of regularisation by weighing the terms:\n",
    "vae_loss = rec_loss + 0.3 * kl_loss\n",
    "\n",
    "VAE.add_loss(vae_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VAE.compile(optimizer='sgd')\n",
    "VAE.fit(X, X, epochs=10, batch_size=32, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the encoder and predict the probabilistic latent embedding in Z:\n",
    "\n",
    "encoder = keras.Model(inp, enc)\n",
    "Z_vae = encoder.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(Z_vae, 'Cell types - deep VAE (2D)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can also have a look at the deterministic mean output:\n",
    "\n",
    "encoder_mean = keras.Model(inp, enc_mean)\n",
    "Z_mean = encoder_mean.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(Z_mean, 'Cell types - deep VAE (2D) - mean')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}